# -*- coding: utf-8 -*-
"""DEEP lEARING FOR PREDICCTING OSG .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FbuSMyNpeZRNRnxDgklxm54X2HKtNmqA

üß† Obstructive Sleep Apnea Risk Detection Using the Aariz Cephalometric Dataset

This project aims to explore the potential of craniofacial structure analysis in predicting the risk of Obstructive Sleep Apnea (OSA) using the publicly available Aariz cephalometric dataset. The dataset contains 1,000 lateral cephalometric radiographs annotated with 29 key anatomical landmarks by expert orthodontists.

üîç Objective:

To develop a machine learning pipeline that processes cephalometric X-rays and anatomical landmarks to extract diagnostic features potentially associated with OSA risk.

üîß Workflow:

Data Loading: Load and preprocess cephalometric images and 29-point landmark annotations from the Aariz dataset.

Feature Extraction or Deep Learning:

Option A: Train a CNN on cephalograms for predictive modeling (when OSA labels are available).

Option B: Extract geometric features (e.g. distances, angles) from landmarks for classical ML.

Model Training: Use extracted features or image features to train a classifier (or pretrain the network).

Evaluation: Prepare the model pipeline for future evaluation using OSA diagnostic labels (e.g., AHI).

Visualization: Overlay predicted landmarks or visualize CNN saliency for interpretation.

importing libaries
"""

import os
import json
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

"""Loading dataset"""

from google.colab import drive
drive.mount('/content/drive')

"""set the path as per folder"""

import os

# CHANGE this path if your folder name is slightly different
aariz_root = '/content/drive/MyDrive/Aariz'
base_dir = os.path.join(aariz_root, 'train')

# Subfolders
image_dir = os.path.join(base_dir, 'Cephalograms')
landmark_dir = os.path.join(base_dir, 'Annotations', 'Cephalometric Landmarks', 'Senior')

"""Load a Sample Image

Use PIL to open one X-ray:
"""

from PIL import Image
import matplotlib.pyplot as plt
import json
import os

# Get one sample
image_files = os.listdir(image_dir)
if not image_files:
    print("No image files found in the specified directory.")
else:
    sample_img = image_files[0]
    img_path = os.path.join(image_dir, sample_img)

    # Load image
    image = Image.open(img_path).convert('L')

    # Load landmark JSON
    json_name = sample_img.replace('.png', '.json')
    json_path = os.path.join(landmark_dir, json_name)

    if os.path.exists(json_path):
        with open(json_path, 'r') as f:
            landmark_data = json.load(f)
        landmarks = [(lm['x'], lm['y']) for lm in landmark_data['landmarks']]

        # Show image + landmarks
        plt.figure(figsize=(6,6))
        plt.imshow(image, cmap='gray')
        for (x, y) in landmarks:
            plt.scatter(x, y, c='red', s=10)
        plt.title(sample_img)
        plt.axis('off')
        plt.show()
    else:
        print(f"Landmark file not found for {sample_img} at {json_path}")

import os

image_files = os.listdir(image_dir)
missing_annotations = []

for img_file in image_files:
    json_name = img_file.replace('.png', '.json')
    json_path = os.path.join(landmark_dir, json_name)
    if not os.path.exists(json_path):
        missing_annotations.append(img_file)

if missing_annotations:
    print(f"Found {len(missing_annotations)} images with missing annotations:")
    for missing_img in missing_annotations:
        print(missing_img)
else:
    print("All image files have corresponding annotation files.")

"""Step 1: Install and Import Required Libraries"""

!pip install torch torchvision --quiet

import os
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

"""define class labels"""

import random
class AarizImageDataset(Dataset):
    def __init__(self, image_folder, transform=None):
        self.image_folder = image_folder
        self.image_files = sorted(os.listdir(image_folder))
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_name = self.image_files[idx]
        image_path = os.path.join(self.image_folder, image_name)
        image = Image.open(image_path).convert('RGB')  # 3-channel

        if self.transform:
            image = self.transform(image)

        label = random.choice([0, 1]) # Placeholder (until OSA labels are available)
        return image, label

"""Set Image Paths and Transforms"""

# Replace this with your exact path if needed
train_dir = '/content/drive/MyDrive/Aariz/train/Cephalograms'
valid_dir = '/content/drive/MyDrive/Aariz/valid/Cephalograms'

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

"""Load Data"""

train_data = AarizImageDataset(train_dir, transform)
valid_data = AarizImageDataset(valid_dir, transform)

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
valid_loader = DataLoader(valid_data, batch_size=16)

"""Load and Modify Pretrained Model"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # Binary output
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

"""Train the Model"""

for epoch in range(5):  # Increase epochs for real training
    model.train()
    total_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/3], Loss: {total_loss:.4f}")

"""Validate the Model"""

model.eval()
correct, total = 0, 0
with torch.no_grad():
    for images, labels in valid_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

print(f"Validation Accuracy: {(100 * correct / total):.2f}%")

torch.save(model.state_dict(), 'resnet18_osa_demo.pth')

"""Sample Predictions (Visualization)"""

import numpy as np

model.eval()
images, labels = next(iter(valid_loader))
outputs = model(images.to(device))
_, preds = torch.max(outputs, 1)

# Unnormalize for display
def imshow(inp, title=None):
    inp = inp.numpy().transpose((1, 2, 0))
    inp = np.clip(inp * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406], 0, 1)
    plt.imshow(inp)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# Show first 4 predictions
for i in range(4):
    label = 'OSA' if preds[i].item() == 1 else 'Non-OSA'
    imshow(images[i].cpu(), title=f"Predicted: {label}")

import os
import zipfile
from google.colab import files

# Define the directory where visualizations are saved
save_dir = "grad_cam_visualizations"
zip_filename = "grad_cam_visualizations.zip"

# Create a ZipFile object
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    # Iterate over all the files in the directory
    for root, dirs, files_in_dir in os.walk(save_dir):
        for file in files_in_dir:
            # Create a full path to the file
            file_path = os.path.join(root, file)
            # Add file to zip archive, preserving directory structure within the zip
            zipf.write(file_path, os.path.relpath(file_path, save_dir))

print(f"All Grad-CAM visualizations have been zipped into '{zip_filename}'")

# Provide the zip file for download
files.download(zip_filename)

import torch

model.eval()
osa_count = 0
non_osa_count = 0
total_images = 0

with torch.no_grad():
    for images, labels in valid_loader:
        images = images.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        osa_count += (predicted == 1).sum().item()
        non_osa_count += (predicted == 0).sum().item()
        total_images += labels.size(0)

print(f"Total images in validation set: {total_images}")
print(f"Predicted as OSA: {osa_count}")
print(f"Predicted as Non-OSA: {non_osa_count}")
print(f"Predicted as Non-OSA: {non_osa_count}")

import torch.nn.functional as F

model.eval()
with torch.no_grad():
    images, labels = next(iter(valid_loader)) # Get a batch of images
    images = images.to(device)

    outputs = model(images)
    probabilities = F.softmax(outputs, dim=1)

    # For each image in the batch, print the probability of being OSA
    print("Predicted OSA Risk Level (Probability):")
    for i in range(images.size(0)):
        # Assuming class 1 is OSA and class 0 is Non-OSA
        osa_probability = probabilities[i, 1].item() * 100
        print(f"Image {i+1}: {osa_probability:.2f}%")

"""Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique that visualizes which parts of an image a CNN focuses on when making a prediction.

üîç How It Works:

It computes gradients of the predicted class score with respect to the feature maps of the last convolutional layer.

These gradients are then weighted and combined to create a heatmap.

The heatmap shows the important regions in the image that influenced the prediction.

‚úÖ Why It‚Äôs Important for Your Project:

You're dealing with medical imaging (cephalograms).

Doctors and evaluators want to see why the model thinks an image shows OSA.

Grad-CAM makes your ResNet18 predictions interpretable and trustworthy.

This boosts your final submission with real-world relevance.

Define Grad-CAM Class
"""

import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model.eval()
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self.hook_layers()

    def hook_layers(self):
        def forward_hook(module, input, output):
            self.activations = output.detach()

        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].detach()

        self.target_layer.register_forward_hook(forward_hook)
        self.target_layer.register_backward_hook(backward_hook)

    def generate_cam(self, input_tensor, class_idx=None):
        output = self.model(input_tensor)
        if class_idx is None:
            class_idx = output.argmax(dim=1).item()

        self.model.zero_grad()
        class_score = output[0, class_idx]
        class_score.backward()

        weights = self.gradients.mean(dim=[2, 3], keepdim=True)
        cam = (weights * self.activations).sum(dim=1).squeeze()

        cam = torch.clamp(cam, min=0)
        cam = cam / cam.max()
        return cam.cpu().numpy()

import os
import zipfile
from google.colab import files

# Define the directory where visualizations are saved
save_dir = "grad_cam_visualizations"
zip_filename = "grad_cam_visualizations.zip"

# Create a ZipFile object
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    # Iterate over all the files in the directory
    for root, dirs, files_in_dir in os.walk(save_dir):
        for file in files_in_dir:
            # Create a full path to the file
            file_path = os.path.join(root, file)
            # Add file to zip archive, preserving directory structure within the zip
            zipf.write(file_path, os.path.relpath(file_path, save_dir))

print(f"All Grad-CAM visualizations have been zipped into '{zip_filename}'")

# Provide the zip file for download
files.download(zip_filename)

"""Load Image and Run Grad-CAM"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
import os

model.eval()

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    # Unnormalize for display
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title:
        plt.title(title)
    plt.axis('off')

# Attach Grad-CAM to the last convolution layer of ResNet
target_layer = model.layer4[-1] # This assumes model.layer4 is the last conv block
gradcam = GradCAM(model, target_layer)

print("Generating and saving all Predicted OSA vs. Non-OSA Grad-CAM overlays...")

# Create a directory to save the visualizations
save_dir = "grad_cam_visualizations"
os.makedirs(save_dir, exist_ok=True)

image_count = 0
for images, labels in valid_loader:
    images = images.to(device)

    # Get model predictions
    outputs = model(images)
    _, preds = torch.max(outputs, 1)

    for i in range(len(images)):
        img_tensor = images[i].unsqueeze(0) # Add batch dimension
        predicted_class_idx = preds[i].item()
        predicted_label = 'OSA' if predicted_class_idx == 1 else 'Non-OSA'

        # Generate Grad-CAM for the predicted class
        cam = gradcam.generate_cam(img_tensor, class_idx=predicted_class_idx)

        # Create overlay
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        heatmap = cv2.resize(heatmap, (224, 224))
        heatmap = np.float32(heatmap) / 255

        original_img_np = images[i].cpu() # Move to CPU for numpy conversion
        original_img_np = original_img_np.numpy().transpose((1, 2, 0))
        # Unnormalize for display
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        original_img_np = std * original_img_np + mean
        original_img_np = np.clip(original_img_np, 0, 1)

        overlay = heatmap + original_img_np
        overlay = overlay / np.max(overlay)

        plt.figure(figsize=(10, 5))
        plt.suptitle(f"Predicted: {predicted_label}", fontsize=12)

        plt.subplot(1, 2, 1)
        plt.imshow(original_img_np)
        plt.title("Original Image")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(overlay)
        plt.title("Grad-CAM Overlay")
        plt.axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

        # Save the plot with a unique filename
        filename = f"grad_cam_prediction_{image_count}_{predicted_label}.png"
        filepath = os.path.join(save_dir, filename)
        plt.savefig(filepath)
        plt.close() # Close the plot to free up memory

        image_count += 1

print(f"Successfully generated and saved {image_count} Grad-CAM visualizations to '{save_dir}'.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import torch.nn.functional as F

model.eval()

all_labels = []
all_predictions = []
all_probabilities = []

with torch.no_grad():
    for images, labels in valid_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        probabilities = F.softmax(outputs, dim=1)

        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())
        all_probabilities.extend(probabilities[:, 1].cpu().numpy()) # Probability of the positive class (OSA)

# Convert lists to numpy arrays
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)
all_probabilities = np.array(all_probabilities)

# Calculate metrics
accuracy = accuracy_score(all_labels, all_predictions)
precision = precision_score(all_labels, all_predictions, average='binary', zero_division=0)
recall = recall_score(all_labels, all_predictions, average='binary', zero_division=0)
f1 = f1_score(all_labels, all_predictions, average='binary', zero_division=0)
conf_matrix = confusion_matrix(all_labels, all_predictions)

print(f"\nQuantitative Model Evaluation on Validation Set:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"\nConfusion Matrix:\n{conf_matrix}")



"""These metrics provide a comprehensive view of the model's performance:

*   **Accuracy**: The proportion of correctly classified instances overall.
*   **Precision**: The proportion of positive identifications that were actually correct. High precision means fewer false positives.
*   **Recall**: The proportion of actual positives that were identified correctly. High recall means fewer false negatives.
*   **F1-Score**: The harmonic mean of precision and recall, offering a balance between the two. It's especially useful when dealing with imbalanced classes.
*   **Confusion Matrix**: A table showing the number of true positives, true negatives, false positives, and false negatives.
"""